{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb919fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from xgboost import XGBRegressor\n",
    "import time\n",
    "import ast\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "import decimal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914971f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d24b7750",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PATH = '/Users/veersingh/Desktop/competition_files/'\n",
    "SAVE_MODEL_PATH = '/Users/veersingh/Desktop/Recommendation-System-to-predict-Yelp-ratings/NN_few_feats/model_nn_few.h5'\n",
    "\n",
    "TRAIN_FILE_PATH = FOLDER_PATH + 'yelp_train.csv'\n",
    "BUSINESS_FILE_PATH = FOLDER_PATH + 'business.json'\n",
    "CHECKIN_FILE_PATH = FOLDER_PATH + 'checkin.json'\n",
    "PHOTO_FILE_PATH = FOLDER_PATH + 'photo.json'\n",
    "TIP_FILE_PATH = FOLDER_PATH + 'tip.json'\n",
    "USER_FILE_PATH = FOLDER_PATH + 'user.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b994c6",
   "metadata": {},
   "source": [
    "# Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a6dc1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training dataset. Remove the header and convert a csv string into a list of 3 elements\n",
    "# [user_id, business_id, rating(float type)]\n",
    "train_RDD = sc.textFile(TRAIN_FILE_PATH)\n",
    "headers_train = train_RDD.first()\n",
    "train_RDD = train_RDD.filter(lambda x:x!=headers_train).map(lambda x:x.split(',')).map(lambda x:[x[0], x[1], float(x[2])])\n",
    "\n",
    "#----------- Functions for feature extraction\n",
    "def get_latitude(latitude_value):\n",
    "    if not latitude_value:\n",
    "        return 0\n",
    "    return latitude_value\n",
    "\n",
    "def get_longitude(longitude_value):\n",
    "    if not longitude_value:\n",
    "        return 0\n",
    "    return longitude_value\n",
    "\n",
    "def get_num_attributes(attributes_dict):\n",
    "    if not attributes_dict:\n",
    "        return 0\n",
    "    return len(attributes_dict)\n",
    "\n",
    "def get_rate_true_attributes(attributes_dict):\n",
    "    if not attributes_dict:\n",
    "        return 0\n",
    "    num_total = 0\n",
    "    num_true = 0\n",
    "    for k,v in attributes_dict.items():\n",
    "        if v in ('True', 'False'):\n",
    "            num_total += 1\n",
    "            if v == 'True':\n",
    "                num_true += 1\n",
    "    if num_total == 0:\n",
    "        return 0\n",
    "    return num_true/num_total\n",
    "            \n",
    "def get_num_categories(categories):\n",
    "    if not categories:\n",
    "        return 0\n",
    "    categories = categories.split(',')\n",
    "    return len(categories)\n",
    "\n",
    "def get_num_checkins(checkin_data):\n",
    "    return sum(checkin_data.values())\n",
    "\n",
    "def get_yelping_since(yelping_since):\n",
    "    date_obj = datetime.strptime(yelping_since, '%Y-%m-%d')\n",
    "    utc_date = pytz.utc.localize(date_obj)\n",
    "    return int(utc_date.timestamp())\n",
    "\n",
    "def get_num_friends(friends):\n",
    "    if friends == 'None':\n",
    "        return 0\n",
    "    friends = friends.split(',')\n",
    "    return len(friends)\n",
    "\n",
    "def get_num_elites(elite):\n",
    "    if elite == 'None':\n",
    "        return 0\n",
    "    elite = elite.split(',')\n",
    "    return len(elite)\n",
    "\n",
    "#---------------------------------------------\n",
    "\n",
    "# Get the following features for each business: id, latitude, longitude, stars, review_count, if its open or closed, rate of true attributes i.e. num true attributes/total attributes and number of categories\n",
    "business_RDD = sc.textFile(BUSINESS_FILE_PATH).map(lambda x: json.loads(x)).map(lambda x: (x['business_id'],\n",
    "                                                                                              [float(get_latitude(x['latitude'])),\n",
    "                                                                                              float(get_longitude(x['longitude'])),\n",
    "                                                                                              float(x['stars']),\n",
    "                                                                                              int(x['review_count']),\n",
    "                                                                                              int(x['is_open']),\n",
    "                                                                                              get_rate_true_attributes(x['attributes']),\n",
    "                                                                                              get_num_categories(x['categories'])]\n",
    "                                                                                          ))\n",
    "\n",
    "# Get the total number of check ins for a business\n",
    "checkIn_RDD = sc.textFile(CHECKIN_FILE_PATH).map(lambda x: json.loads(x)).map(lambda x: (x['business_id'], get_num_checkins(x['time']))).map(lambda x: (x[0], [x[1]]))\n",
    "\n",
    "# Get the total number of photos for a business\n",
    "photo_RDD = sc.textFile(PHOTO_FILE_PATH).map(lambda x: json.loads(x)).map(lambda x: (x['business_id'], 1)).reduceByKey(lambda x,y: x+y).map(lambda x: (x[0], [x[1]]))\n",
    "\n",
    "# Get the total number of tips given by a user and the total number of tips for each business\n",
    "tip_RDD = sc.textFile(TIP_FILE_PATH).map(lambda x: json.loads(x))\n",
    "\n",
    "tips_business_RDD = tip_RDD.map(lambda x: (x['business_id'], 1)).reduceByKey(lambda x,y: x+y).map(lambda x: (x[0], [x[1]]))\n",
    "tips_user_RDD = tip_RDD.map(lambda x: (x['user_id'], 1)).reduceByKey(lambda x,y: x+y).map(lambda x: (x[0], [x[1]]))\n",
    "\n",
    "# Get the features for each user\n",
    "user_RDD = sc.textFile(USER_FILE_PATH).map(lambda x: json.loads(x)).map(lambda x: (x['user_id'],\n",
    "                                                                               [\n",
    "                                                                                   int(x['review_count']),\n",
    "                                                                                   get_yelping_since(x['yelping_since']),\n",
    "                                                                                   get_num_friends(x['friends']),\n",
    "                                                                                   int(x['useful']),\n",
    "                                                                                   int(x['funny']),\n",
    "                                                                                   int(x['cool']),\n",
    "                                                                                   int(x['fans']),\n",
    "                                                                                   get_num_elites(x['elite']),\n",
    "                                                                                   float(x['average_stars']),\n",
    "                                                                                   int(x['compliment_hot']),\n",
    "                                                                                   int(x['compliment_more']),\n",
    "                                                                                   int(x['compliment_profile']),\n",
    "                                                                                   int(x['compliment_cute']),\n",
    "                                                                                   int(x['compliment_list']),\n",
    "                                                                                   int(x['compliment_note']),\n",
    "                                                                                   int(x['compliment_plain']),\n",
    "                                                                                   int(x['compliment_cool']),\n",
    "                                                                                   int(x['compliment_funny']),\n",
    "                                                                                   int(x['compliment_writer']),\n",
    "                                                                                   int(x['compliment_photos'])\n",
    "                                                                               ]))\n",
    "\n",
    "\n",
    "#----------- Create train X and Y\n",
    "def combine_lists(data_row):\n",
    "    # fix nonetype error\n",
    "    if data_row[1][1] == None:\n",
    "        return[data_row[0], data_row[1][0] + [0]]\n",
    "    if type(data_row[1][0]) == str:\n",
    "        return [data_row[0], [data_row[1][0]] + data_row[1][1]]\n",
    "    return [data_row[0], data_row[1][0] + data_row[1][1]]\n",
    "\n",
    "# Combine the following RDDs to create a vector for each business with business id as key and list of features as value\n",
    "# business_RDD + checkIn_RDD + photo_RDD + tips_business_RDD\n",
    "# make sure to fix NoneType error when combining lists since some values are None\n",
    "business_features_RDD = business_RDD.leftOuterJoin(checkIn_RDD).map(lambda x: combine_lists(x)).leftOuterJoin(photo_RDD).map(lambda x: combine_lists(x)).leftOuterJoin(tips_business_RDD).map(lambda x: combine_lists(x))\n",
    "\n",
    "\n",
    "# Combine the following RDDs to create a vector for each user with user id as key and list of features as value\n",
    "# user_RDD + tips_user_RDD\n",
    "# make sure to fix NoneType error when combining lists since some values are None\n",
    "user_features_RDD = user_RDD.leftOuterJoin(tips_user_RDD).map(lambda x: combine_lists(x))\n",
    "\n",
    "def switch_keys(data_row):\n",
    "    bus_id = data_row[0]\n",
    "    usr_id = data_row[1][0]\n",
    "    features = data_row[1][1:]\n",
    "    \n",
    "    return (usr_id, [bus_id] + features)\n",
    "\n",
    "def join_all(data_row):\n",
    "    usr_id = data_row[0]\n",
    "    bus_id = data_row[1][0][0]\n",
    "    bus_features = data_row[1][0][1:]\n",
    "    usr_features = data_row[1][1]\n",
    "    \n",
    "    return ((usr_id, bus_id), bus_features + usr_features)\n",
    "\n",
    "# join the train_RDD and business_features_RDD\n",
    "# we need to have the business_id as the key for this\n",
    "train_RDD_tmp = train_RDD.map(lambda x: (x[1], x[0]))\n",
    "train_join_business_features_RDD = train_RDD_tmp.leftOuterJoin(business_features_RDD).map(lambda x: combine_lists(x))\n",
    "\n",
    "# now join this with the user_features_RDD. We need to have the user_id as key for this\n",
    "train_join_business_features_RDD_tmp = train_join_business_features_RDD.map(lambda x: switch_keys(x))\n",
    "train_join_business_features_user_features_RDD = train_join_business_features_RDD_tmp.leftOuterJoin(user_features_RDD)\n",
    "\n",
    "# format the data as (user_id, business_id) [feature1, feature2, ...]\n",
    "train_all_joined_MAP = train_join_business_features_user_features_RDD.map(lambda x: join_all(x)).collectAsMap()\n",
    "\n",
    "# get the values in trainRDD\n",
    "labels_MAP = train_RDD.map(lambda x: ((x[0], x[1]), x[2])).collectAsMap()\n",
    "\n",
    "# create the x and y training lists\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for k in train_all_joined_MAP:\n",
    "    x_train.append(train_all_joined_MAP[k])\n",
    "    y_train.append(labels_MAP[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ba38b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455854"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8219502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455854"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ee39f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create an instance of MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Assuming your original feature vector is stored in X\n",
    "# X should be a 2D array or dataframe, where each row represents a data point and each column represents a feature\n",
    "\n",
    "# Fit the scaler to the data to compute the min and max values for each feature\n",
    "scaler.fit(x_train)\n",
    "\n",
    "# Transform the features to normalized values\n",
    "x_train_normalized = scaler.transform(x_train)\n",
    "\n",
    "# X_normalized now contains the normalized feature values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d220484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36.1571926, -115.2930234, 4.0, 243, 1, 0.6, 5, 1270, 12, 73, 65, 1325203200, 12, 3, 1, 0, 3, 0, 4.77, 0, 1, 0, 0, 0, 0, 1, 3, 3, 3, 0, 28]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "579d3268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.15632654e-01 5.18814240e-03 7.50000000e-01 2.68329554e-02\n",
      " 1.00000000e+00 6.00000000e-01 2.63157895e-01 9.17119811e-03\n",
      " 1.04347826e-02 2.03853672e-02 2.85450424e-03 5.32861476e-01\n",
      " 1.30250733e-03 1.48663515e-05 5.97514340e-06 0.00000000e+00\n",
      " 1.33037694e-03 0.00000000e+00 9.35754190e-01 0.00000000e+00\n",
      " 4.04040404e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 6.29049506e-05 1.60737248e-04 1.60737248e-04\n",
      " 3.76317110e-04 0.00000000e+00 1.77777778e-02]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_normalized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7bfcb",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d546e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PATH = '/Users/veersingh/Desktop/competition_files/'\n",
    "TESTING_FILE_PATH = '/Users/veersingh/Desktop/competition_files/yelp_val.csv'\n",
    "OUTPUT_FILE_PATH = '/Users/veersingh/Desktop/Recommendation-System-to-predict-Yelp-ratings/output_NN_few.csv'\n",
    "\n",
    "BUSINESS_FILE_PATH = FOLDER_PATH + 'business.json'\n",
    "CHECKIN_FILE_PATH = FOLDER_PATH + 'checkin.json'\n",
    "PHOTO_FILE_PATH = FOLDER_PATH + 'photo.json'\n",
    "TIP_FILE_PATH = FOLDER_PATH + 'tip.json'\n",
    "USER_FILE_PATH = FOLDER_PATH + 'user.json'\n",
    "\n",
    "# MODEL_FILE_PATH = '/Users/veersingh/Desktop/Recommendation-System-to-predict-Yelp-ratings/NN_few_feats/model_nn_few.h5'\n",
    "# LOADED_MODEL = load_model(MODEL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1171703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- Functions for feature extraction\n",
    "def get_latitude(latitude_value):\n",
    "    if not latitude_value:\n",
    "        return 0\n",
    "    return latitude_value\n",
    "\n",
    "def get_longitude(longitude_value):\n",
    "    if not longitude_value:\n",
    "        return 0\n",
    "    return longitude_value\n",
    "\n",
    "def get_num_attributes(attributes_dict):\n",
    "    if not attributes_dict:\n",
    "        return 0\n",
    "    return len(attributes_dict)\n",
    "\n",
    "def get_rate_true_attributes(attributes_dict):\n",
    "    if not attributes_dict:\n",
    "        return 0\n",
    "    num_total = 0\n",
    "    num_true = 0\n",
    "    for k,v in attributes_dict.items():\n",
    "        if v in ('True', 'False'):\n",
    "            num_total += 1\n",
    "            if v == 'True':\n",
    "                num_true += 1\n",
    "    if num_total == 0:\n",
    "        return 0\n",
    "    return num_true/num_total\n",
    "            \n",
    "def get_num_categories(categories):\n",
    "    if not categories:\n",
    "        return 0\n",
    "    categories = categories.split(',')\n",
    "    return len(categories)\n",
    "\n",
    "def get_num_checkins(checkin_data):\n",
    "    return sum(checkin_data.values())\n",
    "\n",
    "def get_yelping_since(yelping_since):\n",
    "    date_obj = datetime.strptime(yelping_since, '%Y-%m-%d')\n",
    "    utc_date = pytz.utc.localize(date_obj)\n",
    "    return int(utc_date.timestamp())\n",
    "\n",
    "def get_num_friends(friends):\n",
    "    if friends == 'None':\n",
    "        return 0\n",
    "    friends = friends.split(',')\n",
    "    return len(friends)\n",
    "\n",
    "def get_num_elites(elite):\n",
    "    if elite == 'None':\n",
    "        return 0\n",
    "    elite = elite.split(',')\n",
    "    return len(elite)\n",
    "\n",
    "#---------------------------------------------\n",
    "\n",
    "# Get the following features for each business: id, latitude, longitude, stars, review_count, if its open or closed, rate of true attributes i.e. num true attributes/total attributes and number of categories\n",
    "business_RDD = sc.textFile(BUSINESS_FILE_PATH).map(lambda x: json.loads(x)).map(lambda x: (x['business_id'],\n",
    "                                                                                              [float(get_latitude(x['latitude'])),\n",
    "                                                                                              float(get_longitude(x['longitude'])),\n",
    "                                                                                              float(x['stars']),\n",
    "                                                                                              int(x['review_count']),\n",
    "                                                                                              int(x['is_open']),\n",
    "                                                                                              get_rate_true_attributes(x['attributes']),\n",
    "                                                                                              get_num_categories(x['categories'])]\n",
    "                                                                                          ))\n",
    "\n",
    "# Get the total number of check ins for a business\n",
    "checkIn_RDD = sc.textFile(CHECKIN_FILE_PATH).map(lambda x: json.loads(x)).map(lambda x: (x['business_id'], get_num_checkins(x['time']))).map(lambda x: (x[0], [x[1]]))\n",
    "\n",
    "# Get the total number of photos for a business\n",
    "photo_RDD = sc.textFile(PHOTO_FILE_PATH).map(lambda x: json.loads(x)).map(lambda x: (x['business_id'], 1)).reduceByKey(lambda x,y: x+y).map(lambda x: (x[0], [x[1]]))\n",
    "\n",
    "# Get the total number of tips given by a user and the total number of tips for each business\n",
    "tip_RDD = sc.textFile(TIP_FILE_PATH).map(lambda x: json.loads(x))\n",
    "\n",
    "tips_business_RDD = tip_RDD.map(lambda x: (x['business_id'], 1)).reduceByKey(lambda x,y: x+y).map(lambda x: (x[0], [x[1]]))\n",
    "tips_user_RDD = tip_RDD.map(lambda x: (x['user_id'], 1)).reduceByKey(lambda x,y: x+y).map(lambda x: (x[0], [x[1]]))\n",
    "\n",
    "# Get the features for each user\n",
    "user_RDD = sc.textFile(USER_FILE_PATH).map(lambda x: json.loads(x)).map(lambda x: (x['user_id'],\n",
    "                                                                               [\n",
    "                                                                                   int(x['review_count']),\n",
    "                                                                                   get_yelping_since(x['yelping_since']),\n",
    "                                                                                   get_num_friends(x['friends']),\n",
    "                                                                                   int(x['useful']),\n",
    "                                                                                   int(x['funny']),\n",
    "                                                                                   int(x['cool']),\n",
    "                                                                                   int(x['fans']),\n",
    "                                                                                   get_num_elites(x['elite']),\n",
    "                                                                                   float(x['average_stars']),\n",
    "                                                                                   int(x['compliment_hot']),\n",
    "                                                                                   int(x['compliment_more']),\n",
    "                                                                                   int(x['compliment_profile']),\n",
    "                                                                                   int(x['compliment_cute']),\n",
    "                                                                                   int(x['compliment_list']),\n",
    "                                                                                   int(x['compliment_note']),\n",
    "                                                                                   int(x['compliment_plain']),\n",
    "                                                                                   int(x['compliment_cool']),\n",
    "                                                                                   int(x['compliment_funny']),\n",
    "                                                                                   int(x['compliment_writer']),\n",
    "                                                                                   int(x['compliment_photos'])\n",
    "                                                                               ]))\n",
    "\n",
    "\n",
    "#----------- Create train X and Y\n",
    "def combine_lists(data_row):\n",
    "    # fix nonetype error\n",
    "    if data_row[1][1] == None:\n",
    "        return[data_row[0], data_row[1][0] + [0]]\n",
    "    if type(data_row[1][0]) == str:\n",
    "        return [data_row[0], [data_row[1][0]] + data_row[1][1]]\n",
    "    return [data_row[0], data_row[1][0] + data_row[1][1]]\n",
    "\n",
    "# Combine the following RDDs to create a vector for each business with business id as key and list of features as value\n",
    "# business_RDD + checkIn_RDD + photo_RDD + tips_business_RDD\n",
    "# make sure to fix NoneType error when combining lists since some values are None\n",
    "business_features_RDD = business_RDD.leftOuterJoin(checkIn_RDD).map(lambda x: combine_lists(x)).leftOuterJoin(photo_RDD).map(lambda x: combine_lists(x)).leftOuterJoin(tips_business_RDD).map(lambda x: combine_lists(x))\n",
    "\n",
    "\n",
    "# Combine the following RDDs to create a vector for each user with user id as key and list of features as value\n",
    "# user_RDD + tips_user_RDD\n",
    "# make sure to fix NoneType error when combining lists since some values are None\n",
    "user_features_RDD = user_RDD.leftOuterJoin(tips_user_RDD).map(lambda x: combine_lists(x))\n",
    "\n",
    "def switch_keys(data_row):\n",
    "    bus_id = data_row[0]\n",
    "    usr_id = data_row[1][0]\n",
    "    features = data_row[1][1:]\n",
    "    \n",
    "    return (usr_id, [bus_id] + features)\n",
    "\n",
    "def join_all(data_row):\n",
    "    usr_id = data_row[0]\n",
    "    bus_id = data_row[1][0][0]\n",
    "    bus_features = data_row[1][0][1:]\n",
    "    usr_features = data_row[1][1]\n",
    "    \n",
    "    return ((usr_id, bus_id), bus_features + usr_features)\n",
    "\n",
    "#----------- Testing Phase -----------\n",
    "# Read in the testing dataset. Remove the header and convert a csv string into a list of 2 elements\n",
    "# [user_id, business_id]\n",
    "test_RDD = sc.textFile(TESTING_FILE_PATH)\n",
    "headers_test = test_RDD.first()\n",
    "test_RDD = test_RDD.filter(lambda x:x!=headers_test).map(lambda x:x.split(',')).map(lambda x:(x[0], x[1]))\n",
    "\n",
    "# join the test_RDD and business_features_RDD\n",
    "# we need to have the business_id as the key for this\n",
    "test_RDD_tmp = test_RDD.map(lambda x: (x[1], x[0]))\n",
    "test_join_business_features_RDD = test_RDD_tmp.leftOuterJoin(business_features_RDD).map(lambda x: combine_lists(x))\n",
    "\n",
    "# now join this with the user_features_RDD. We need to have the user_id as key for this\n",
    "test_join_business_features_RDD_tmp = test_join_business_features_RDD.map(lambda x: switch_keys(x))\n",
    "test_join_business_features_user_features_RDD = test_join_business_features_RDD_tmp.leftOuterJoin(user_features_RDD)\n",
    "\n",
    "# format the data as (user_id, business_id) [feature1, feature2, ...]\n",
    "test_all_joined_MAP = test_join_business_features_user_features_RDD.map(lambda x: join_all(x)).collectAsMap()\n",
    "\n",
    "# create the x testing list\n",
    "x_test = []\n",
    "test_labels = []\n",
    "for k in test_all_joined_MAP:\n",
    "    x_test.append(test_all_joined_MAP[k])\n",
    "    test_labels.append(k)\n",
    "#--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "797b046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create y_test for the target labels of the validation data\n",
    "validation_targets = sc.textFile(TESTING_FILE_PATH)\n",
    "header = validation_targets.first()\n",
    "validation_targets = validation_targets.filter(lambda x: x!=header).map(lambda x: x.split(','))\n",
    "validation_targets_RDD = validation_targets.map(lambda x: ((x[0], x[1]), x[2]))\n",
    "validation_targets_MAP = validation_targets_RDD.collectAsMap()\n",
    "\n",
    "y_test = []\n",
    "\n",
    "for usr_bus in test_labels:\n",
    "    y_test.append(float(validation_targets_MAP[usr_bus]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d03ce0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142044"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "628c1dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142044"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2279b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create an instance of MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Assuming your original feature vector is stored in X\n",
    "# X should be a 2D array or dataframe, where each row represents a data point and each column represents a feature\n",
    "\n",
    "# Fit the scaler to the data to compute the min and max values for each feature\n",
    "scaler.fit(x_test)\n",
    "\n",
    "# Transform the features to normalized values\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "# X_normalized now contains the normalized feature values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce005b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35.2205594, -80.9438737, 3.5, 1732, 1, 0, 2, 52675, 0, 934, 291, 1267228800, 31, 14, 4, 3, 7, 5, 3.41, 0, 7, 2, 0, 0, 7, 8, 12, 12, 11, 0, 22]\n"
     ]
    }
   ],
   "source": [
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20a43123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.78111196e-02 1.53512529e-01 6.25000000e-01 2.14411691e-01\n",
      " 1.00000000e+00 0.00000000e+00 1.05263158e-01 3.80388079e-01\n",
      " 0.00000000e+00 2.60821000e-01 2.18285618e-02 3.97168857e-01\n",
      " 3.36481059e-03 6.93763070e-05 2.39005736e-05 1.44697558e-05\n",
      " 3.10421286e-03 3.57142857e-01 5.55865922e-01 0.00000000e+00\n",
      " 2.82828283e-03 7.28332119e-04 0.00000000e+00 0.00000000e+00\n",
      " 9.61538462e-04 5.03239605e-04 6.42948993e-04 6.42948993e-04\n",
      " 1.37982940e-03 0.00000000e+00 1.39682540e-02]\n"
     ]
    }
   ],
   "source": [
    "print(x_test_normalized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba6359c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Wz6nekYXj4wZ39UKgaVHJA', 'yQab5dxZzgBLTEHCw9V7_w')\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5374769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142044\n"
     ]
    }
   ],
   "source": [
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c83d828d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37763608",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb52ed9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend changed to:  tensorflow\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# Change the backend to TensorFlow\n",
    "K._BACKEND = 'tensorflow'\n",
    "\n",
    "# Verify the backend has been changed\n",
    "backend_name = K._BACKEND\n",
    "print(\"Backend changed to: \", backend_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e0adfe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_34 (Dense)                 (None, 64)            2048        dense_input_10[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_35 (Dense)                 (None, 32)            2080        dense_34[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_36 (Dense)                 (None, 1)             33          dense_35[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 4161\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the neural network model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the input layer with the same input shape as your feature vectors (1423)\n",
    "model.add(Dense(64, input_dim=31, activation='relu'))  # Example hidden layer with 64 units and ReLU activation\n",
    "\n",
    "# Add additional hidden layers as needed\n",
    "model.add(Dense(32, activation='relu'))  # Example additional hidden layer with 32 units and ReLU activation\n",
    "\n",
    "# Add the output layer with a single output unit for regression or multiple output units for classification\n",
    "model.add(Dense(1, activation='linear'))  # Example output layer with a single output unit for regression\n",
    "\n",
    "# Compile the model\n",
    "# Example compilation with Adam optimizer and mean squared error loss for regression\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ddf2c8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 455854 samples, validate on 142044 samples\n",
      "Epoch 1/20\n",
      "5s - loss: 1.0201 - val_loss: 0.9767\n",
      "Epoch 2/20\n",
      "5s - loss: 0.9836 - val_loss: 0.9831\n",
      "Epoch 3/20\n",
      "4s - loss: 0.9813 - val_loss: 0.9713\n",
      "Epoch 4/20\n",
      "4s - loss: 0.9804 - val_loss: 0.9777\n",
      "Epoch 5/20\n",
      "4s - loss: 0.9797 - val_loss: 0.9741\n",
      "Epoch 6/20\n",
      "5s - loss: 0.9789 - val_loss: 0.9755\n",
      "Epoch 7/20\n",
      "5s - loss: 0.9781 - val_loss: 0.9718\n",
      "Epoch 8/20\n",
      "5s - loss: 0.9778 - val_loss: 0.9737\n",
      "Epoch 9/20\n",
      "5s - loss: 0.9771 - val_loss: 0.9697\n",
      "Epoch 10/20\n",
      "5s - loss: 0.9764 - val_loss: 0.9733\n",
      "Epoch 11/20\n",
      "5s - loss: 0.9767 - val_loss: 0.9711\n",
      "Epoch 12/20\n",
      "5s - loss: 0.9764 - val_loss: 0.9711\n",
      "Epoch 13/20\n",
      "5s - loss: 0.9758 - val_loss: 0.9682\n",
      "Epoch 14/20\n",
      "5s - loss: 0.9756 - val_loss: 0.9735\n",
      "Epoch 15/20\n",
      "5s - loss: 0.9754 - val_loss: 0.9682\n",
      "Epoch 16/20\n",
      "5s - loss: 0.9751 - val_loss: 0.9681\n",
      "Epoch 17/20\n",
      "5s - loss: 0.9749 - val_loss: 0.9684\n",
      "Epoch 18/20\n",
      "5s - loss: 0.9746 - val_loss: 0.9695\n",
      "Epoch 19/20\n",
      "5s - loss: 0.9747 - val_loss: 0.9670\n",
      "Epoch 20/20\n",
      "5s - loss: 0.9742 - val_loss: 0.9682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8321ae6438>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x=x_train_normalized, y=y_train, batch_size=32, nb_epoch=20, verbose=2, validation_data = (x_test_normalized, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03db0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(SAVE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d8ba69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
