{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d947fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from xgboost import XGBRegressor\n",
    "import time\n",
    "import sys\n",
    "from sklearn.model_selection import  GridSearchCV\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40abf750",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e4b1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PATH = '/Users/veersingh/Desktop/competition_files/'\n",
    "TESTING_FILE_PATH = '/Users/veersingh/Desktop/competition_files/yelp_val.csv'\n",
    "OUTPUT_FILE_PATH = '/Users/veersingh/Desktop/Recommendation-System-to-predict-Yelp-ratings/output.csv'\n",
    "\n",
    "TRAIN_FILE_PATH = FOLDER_PATH + 'yelp_train.csv'\n",
    "BUSINESS_FILE_PATH = FOLDER_PATH + 'business.json'\n",
    "CHECKIN_FILE_PATH = FOLDER_PATH + 'checkin.json'\n",
    "PHOTO_FILE_PATH = FOLDER_PATH + 'photo.json'\n",
    "TIP_FILE_PATH = FOLDER_PATH + 'tip.json'\n",
    "USER_FILE_PATH = FOLDER_PATH + 'user.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e00c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training dataset. Remove the header and convert a csv string into a list of 3 elements\n",
    "# [user_id, business_id, rating(float type)]\n",
    "train_RDD = sc.textFile(TRAIN_FILE_PATH)\n",
    "headers_train = train_RDD.first()\n",
    "train_RDD = train_RDD.filter(lambda x:x!=headers_train).map(lambda x:x.split(',')).map(lambda x:[x[0], x[1], float(x[2])])\n",
    "\n",
    "#----------- Functions for feature extraction\n",
    "def get_latitude(latitude_value):\n",
    "    if not latitude_value:\n",
    "        return 0\n",
    "    return latitude_value\n",
    "\n",
    "def get_longitude(longitude_value):\n",
    "    if not longitude_value:\n",
    "        return 0\n",
    "    return longitude_value\n",
    "\n",
    "def get_num_attributes(attributes_dict):\n",
    "    if not attributes_dict:\n",
    "        return 0\n",
    "    return len(attributes_dict)\n",
    "\n",
    "def get_rate_true_attributes(attributes_dict):\n",
    "    if not attributes_dict:\n",
    "        return 0\n",
    "    num_total = 0\n",
    "    num_true = 0\n",
    "    for k,v in attributes_dict.items():\n",
    "        if v in ('True', 'False'):\n",
    "            num_total += 1\n",
    "            if v == 'True':\n",
    "                num_true += 1\n",
    "    if num_total == 0:\n",
    "        return 0\n",
    "    return num_true/num_total\n",
    "            \n",
    "def get_num_categories(categories):\n",
    "    if not categories:\n",
    "        return 0\n",
    "    categories = categories.split(',')\n",
    "    return len(categories)\n",
    "\n",
    "def get_num_checkins(checkin_data):\n",
    "    return sum(checkin_data.values())\n",
    "\n",
    "def get_yelping_since(yelping_since):\n",
    "    date_obj = datetime.strptime(yelping_since, '%Y-%m-%d')\n",
    "    utc_date = pytz.utc.localize(date_obj)\n",
    "    return int(utc_date.timestamp())\n",
    "\n",
    "def get_num_friends(friends):\n",
    "    if friends == 'None':\n",
    "        return 0\n",
    "    friends = friends.split(',')\n",
    "    return len(friends)\n",
    "\n",
    "def get_num_elites(elite):\n",
    "    if elite == 'None':\n",
    "        return 0\n",
    "    elite = elite.split(',')\n",
    "    return len(elite)\n",
    "\n",
    "#---------------------------------------------\n",
    "\n",
    "# Get the following features for each business: id, latitude, longitude, stars, review_count, if its open or closed, rate of true attributes i.e. num true attributes/total attributes and number of categories\n",
    "business_RDD = sc.textFile(BUSINESS_FILE_PATH).map(lambda x: json.loads(x)).map(lambda x: (x['business_id'],\n",
    "                                                                                              [float(get_latitude(x['latitude'])),\n",
    "                                                                                              float(get_longitude(x['longitude'])),\n",
    "                                                                                              float(x['stars']),\n",
    "                                                                                              int(x['review_count']),\n",
    "                                                                                              int(x['is_open']),\n",
    "                                                                                              get_rate_true_attributes(x['attributes']),\n",
    "                                                                                              get_num_categories(x['categories'])]\n",
    "                                                                                          ))\n",
    "\n",
    "# Get the total number of check ins for a business\n",
    "checkIn_RDD = sc.textFile(CHECKIN_FILE_PATH).map(lambda x: json.loads(x)).map(lambda x: (x['business_id'], get_num_checkins(x['time']))).map(lambda x: (x[0], [x[1]]))\n",
    "\n",
    "# Get the total number of photos for a business\n",
    "photo_RDD = sc.textFile(PHOTO_FILE_PATH).map(lambda x: json.loads(x)).map(lambda x: (x['business_id'], 1)).reduceByKey(lambda x,y: x+y).map(lambda x: (x[0], [x[1]]))\n",
    "\n",
    "# Get the total number of tips given by a user and the total number of tips for each business\n",
    "tip_RDD = sc.textFile(TIP_FILE_PATH).map(lambda x: json.loads(x))\n",
    "\n",
    "tips_business_RDD = tip_RDD.map(lambda x: (x['business_id'], 1)).reduceByKey(lambda x,y: x+y).map(lambda x: (x[0], [x[1]]))\n",
    "tips_user_RDD = tip_RDD.map(lambda x: (x['user_id'], 1)).reduceByKey(lambda x,y: x+y).map(lambda x: (x[0], [x[1]]))\n",
    "\n",
    "# Get the features for each user\n",
    "user_RDD = sc.textFile(USER_FILE_PATH).map(lambda x: json.loads(x)).map(lambda x: (x['user_id'],\n",
    "                                                                               [\n",
    "                                                                                   int(x['review_count']),\n",
    "                                                                                   get_yelping_since(x['yelping_since']),\n",
    "                                                                                   get_num_friends(x['friends']),\n",
    "                                                                                   int(x['useful']),\n",
    "                                                                                   int(x['funny']),\n",
    "                                                                                   int(x['cool']),\n",
    "                                                                                   int(x['fans']),\n",
    "                                                                                   get_num_elites(x['elite']),\n",
    "                                                                                   float(x['average_stars']),\n",
    "                                                                                   int(x['compliment_hot']),\n",
    "                                                                                   int(x['compliment_more']),\n",
    "                                                                                   int(x['compliment_profile']),\n",
    "                                                                                   int(x['compliment_cute']),\n",
    "                                                                                   int(x['compliment_list']),\n",
    "                                                                                   int(x['compliment_note']),\n",
    "                                                                                   int(x['compliment_plain']),\n",
    "                                                                                   int(x['compliment_cool']),\n",
    "                                                                                   int(x['compliment_funny']),\n",
    "                                                                                   int(x['compliment_writer']),\n",
    "                                                                                   int(x['compliment_photos'])\n",
    "                                                                               ]))\n",
    "\n",
    "\n",
    "#----------- Create train X and Y\n",
    "def combine_lists(data_row):\n",
    "    # fix nonetype error\n",
    "    if data_row[1][1] == None:\n",
    "        return[data_row[0], data_row[1][0] + [0]]\n",
    "    if type(data_row[1][0]) == str:\n",
    "        return [data_row[0], [data_row[1][0]] + data_row[1][1]]\n",
    "    return [data_row[0], data_row[1][0] + data_row[1][1]]\n",
    "\n",
    "# Combine the following RDDs to create a vector for each business with business id as key and list of features as value\n",
    "# business_RDD + checkIn_RDD + photo_RDD + tips_business_RDD\n",
    "# make sure to fix NoneType error when combining lists since some values are None\n",
    "business_features_RDD = business_RDD.leftOuterJoin(checkIn_RDD).map(lambda x: combine_lists(x)).leftOuterJoin(photo_RDD).map(lambda x: combine_lists(x)).leftOuterJoin(tips_business_RDD).map(lambda x: combine_lists(x))\n",
    "\n",
    "\n",
    "# Combine the following RDDs to create a vector for each user with user id as key and list of features as value\n",
    "# user_RDD + tips_user_RDD\n",
    "# make sure to fix NoneType error when combining lists since some values are None\n",
    "user_features_RDD = user_RDD.leftOuterJoin(tips_user_RDD).map(lambda x: combine_lists(x))\n",
    "\n",
    "def switch_keys(data_row):\n",
    "    bus_id = data_row[0]\n",
    "    usr_id = data_row[1][0]\n",
    "    features = data_row[1][1:]\n",
    "    \n",
    "    return (usr_id, [bus_id] + features)\n",
    "\n",
    "def join_all(data_row):\n",
    "    usr_id = data_row[0]\n",
    "    bus_id = data_row[1][0][0]\n",
    "    bus_features = data_row[1][0][1:]\n",
    "    usr_features = data_row[1][1]\n",
    "    \n",
    "    return ((usr_id, bus_id), bus_features + usr_features)\n",
    "\n",
    "# join the train_RDD and business_features_RDD\n",
    "# we need to have the business_id as the key for this\n",
    "train_RDD_tmp = train_RDD.map(lambda x: (x[1], x[0]))\n",
    "train_join_business_features_RDD = train_RDD_tmp.leftOuterJoin(business_features_RDD).map(lambda x: combine_lists(x))\n",
    "\n",
    "# now join this with the user_features_RDD. We need to have the user_id as key for this\n",
    "train_join_business_features_RDD_tmp = train_join_business_features_RDD.map(lambda x: switch_keys(x))\n",
    "train_join_business_features_user_features_RDD = train_join_business_features_RDD_tmp.leftOuterJoin(user_features_RDD)\n",
    "\n",
    "# format the data as (user_id, business_id) [feature1, feature2, ...]\n",
    "train_all_joined_MAP = train_join_business_features_user_features_RDD.map(lambda x: join_all(x)).collectAsMap()\n",
    "\n",
    "# get the values in trainRDD\n",
    "labels_MAP = train_RDD.map(lambda x: ((x[0], x[1]), x[2])).collectAsMap()\n",
    "\n",
    "# create the x and y training lists\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for k in train_all_joined_MAP:\n",
    "    x_train.append(train_all_joined_MAP[k])\n",
    "    y_train.append(labels_MAP[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64856728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- Testing Phase -----------\n",
    "# Read in the testing dataset. Remove the header and convert a csv string into a list of 2 elements\n",
    "# [user_id, business_id]\n",
    "test_RDD = sc.textFile(TESTING_FILE_PATH)\n",
    "headers_test = test_RDD.first()\n",
    "test_RDD = test_RDD.filter(lambda x:x!=headers_test).map(lambda x:x.split(',')).map(lambda x:(x[0], x[1]))\n",
    "\n",
    "# join the test_RDD and business_features_RDD\n",
    "# we need to have the business_id as the key for this\n",
    "test_RDD_tmp = test_RDD.map(lambda x: (x[1], x[0]))\n",
    "test_join_business_features_RDD = test_RDD_tmp.leftOuterJoin(business_features_RDD).map(lambda x: combine_lists(x))\n",
    "\n",
    "# now join this with the user_features_RDD. We need to have the user_id as key for this\n",
    "test_join_business_features_RDD_tmp = test_join_business_features_RDD.map(lambda x: switch_keys(x))\n",
    "test_join_business_features_user_features_RDD = test_join_business_features_RDD_tmp.leftOuterJoin(user_features_RDD)\n",
    "\n",
    "# format the data as (user_id, business_id) [feature1, feature2, ...]\n",
    "test_all_joined_MAP = test_join_business_features_user_features_RDD.map(lambda x: join_all(x)).collectAsMap()\n",
    "\n",
    "# create the x testing list\n",
    "x_test = []\n",
    "test_labels = []\n",
    "for k in test_all_joined_MAP:\n",
    "    x_test.append(test_all_joined_MAP[k])\n",
    "    test_labels.append(k)\n",
    "#--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e49aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- Training Phase -----------\n",
    "model = XGBRegressor(learning_rate=0.05,\n",
    "                     max_depth=5,\n",
    "                     min_child_weight=1,\n",
    "                     subsample=0.6,\n",
    "                     colsample_bytree=0.6,\n",
    "                     gamma=0,\n",
    "                     reg_alpha=1,\n",
    "                     reg_lambda=0,\n",
    "                     n_estimators=800)\n",
    "\n",
    "model.fit(X=x_train, y=y_train)\n",
    "#--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d294d1f",
   "metadata": {},
   "source": [
    "#----------- Predictions -----------\n",
    "predictions = model.predict(data=x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e310973",
   "metadata": {},
   "source": [
    "## Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4cc315",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = GridSearchCV(XGBRegressor(),\n",
    "                         {\n",
    "                             'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.3],\n",
    "                             'max_depth': [5],\n",
    "                             'min_child_weight': [1],\n",
    "                             'subsample': [0.6],\n",
    "                             'colsample_bytree': [0.6],\n",
    "                             'gamma': [0],\n",
    "                             'reg_alpha': [1],\n",
    "                             'reg_lambda': [0],\n",
    "                             'n_estimators': [800]\n",
    "                         },\n",
    "                         n_jobs = -1,\n",
    "                         scoring = 'neg_root_mean_squared_error',\n",
    "                         verbose = 10)\n",
    "\n",
    "xgb_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('clf.best_score_', math.sqrt(-1*xgb_model.best_score_))\n",
    "print('clf.best_params_', xgb_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('clf.best_score_', -1*xgb_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9176e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('clf.best_params_', xgb_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2563868",
   "metadata": {},
   "source": [
    "clf.best_params_ {'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.05, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 800, 'reg_alpha': 1, 'reg_lambda': 0, 'subsample': 0.6}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2466f3da",
   "metadata": {},
   "source": [
    "## Max depth and Min child weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0bc9f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/veersingh/opt/anaconda3/envs/dmass1/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/veersingh/opt/anaconda3/envs/dmass1/lib/python3.6/site-packages/sklearn/model_selection/_search.py:925: UserWarning: One or more of the test scores are non-finite: [-0.97600731 -0.96988422 -0.97506715 -0.96881105 -0.97420537 -0.96902581\n",
      " -0.97369534 -0.97022335         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=XGBRegressor(), n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.6], 'gamma': [0],\n",
       "                         'learning_rate': [0.05],\n",
       "                         'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
       "                         'min_child_weight': [0, 1], 'n_estimators': [800],\n",
       "                         'reg_alpha': [1], 'reg_lambda': [0],\n",
       "                         'subsample': [0.6]},\n",
       "             scoring='neg_mean_squared_error', verbose=10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all\n",
    "# 'max_depth': [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# 'min_child_weight': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# first batch\n",
    "# 'max_depth': [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# 'min_child_weight': [0, 1]\n",
    "\n",
    "xgb_model_max_depth_min_child = GridSearchCV(XGBRegressor(),\n",
    "                         {\n",
    "                             'learning_rate': [0.05],\n",
    "                             'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "                             'min_child_weight': [0, 1],\n",
    "                             'subsample': [0.6],\n",
    "                             'colsample_bytree': [0.6],\n",
    "                             'gamma': [0],\n",
    "                             'reg_alpha': [1],\n",
    "                             'reg_lambda': [0],\n",
    "                             'n_estimators': [800]\n",
    "                         },\n",
    "                         n_jobs = -1,\n",
    "                         scoring = 'neg_mean_squared_error',\n",
    "                         verbose = 10)\n",
    "\n",
    "xgb_model_max_depth_min_child.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1019e8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf.best_score_ 0.9842819951901568\n",
      "clf.best_params_ {'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.05, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 800, 'reg_alpha': 1, 'reg_lambda': 0, 'subsample': 0.6}\n"
     ]
    }
   ],
   "source": [
    "print('clf.best_score_', math.sqrt(-1*xgb_model_max_depth_min_child.best_score_))\n",
    "print('clf.best_params_', xgb_model_max_depth_min_child.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057e1b60",
   "metadata": {},
   "source": [
    "clf.best_params_ {'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.05, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 800, 'reg_alpha': 1, 'reg_lambda': 0, 'subsample': 0.6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bba266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all\n",
    "# 'max_depth': [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# 'min_child_weight': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# second batch\n",
    "# 'max_depth': [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# 'min_child_weight': [2]\n",
    "\n",
    "xgb_model_max_depth_min_child = GridSearchCV(XGBRegressor(),\n",
    "                         {\n",
    "                             'learning_rate': [0.05],\n",
    "                             'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "                             'min_child_weight': [0, 1],\n",
    "                             'subsample': [0.6],\n",
    "                             'colsample_bytree': [0.6],\n",
    "                             'gamma': [0],\n",
    "                             'reg_alpha': [1],\n",
    "                             'reg_lambda': [0],\n",
    "                             'n_estimators': [800]\n",
    "                         },\n",
    "                         n_jobs = -1,\n",
    "                         scoring = 'neg_mean_squared_error',\n",
    "                         verbose = 10)\n",
    "\n",
    "xgb_model_max_depth_min_child.fit(x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
